---
title: "MD2_Racinskis"
author: "Pēteris Račinskis pr20015"
date: "5/18/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE)
```


# 2. mājas darbs Mate 6029

## 1. uzdevums - lineāras regresijas, modeļu novērtējumi

Datu kopas ielāde:

``` {r, message=FALSE}
df <- read.table('CMB.dat',header=TRUE)
attach(df)
```

Datu kopa - kosmiskā mikroviļņu fona novērojumi. Svarīgie parametri šajā gadījumā ir 'ell' - multipolu moments (rupji runājot, lenķiskais ekvivalents starojuma frekvencei) un starojuma spektra nobīde 'Cl' (rupji runājot, spektra temperatūras nobīde no vidējā). Pārējās trīs kolonnas ir statistiski novērojumu trokšņa u.c. raksturotāji.

### 1.1. regresijas modeļu ģenerēšana

Vispārināma lineārās regresijas funkcija grafiku zīmēšanai, rezultātu izvadei uz konsoles un darbam ar augstākas pakāpes modeļiem:

``` {r}
general_lreg <- function(vec1,vec2,degree=1,plot=F,print=F,names=c("","")) {
  fit<-lm(vec2~poly(vec1,degree,raw=T)) 
  if(plot){
    plot(vec1,vec2,xlab=names[1],ylab=names[2])
    x <- seq(min(vec1),max(vec1),length.out = length(vec1))
    f <- predict(fit, newdata = data.frame(vec1 = x))
    lines(x,f,col="red",lwd=2)
  }
  if(print){
    print(paste("R-squared:",summary(fit)$r.squared))
  }
  fit
}
```

\newpage
Vienkārša lineārā regresija parametriem 'ell' un 'Cl':

``` {r}
fit1 <- general_lreg(ell,Cl,plot=T,print=T,names=c("ell","Cl"))
```

Funkcija labākās atbilstības polinoma meklēšanai (apstājas, kad ANOVA tests liecina, ka jaunas brīvības pakāpes pievienošana būtisku uzlabojumu modeļa atbilstībā datiem vairs nesniedz). Virzība - pa divām pakāpēm, lai ļautu modelim piekārtoties simetriskiem/asimetriskiem sadalījumiem pēc vajadzības:

``` {r}
bestfit <- function(vec1,vec2,deg=1,last_deg=1,P=0,p=0.05,max=27) {
  if((P>0.05) || (deg>max)){
    list(d=last_deg,p=P)
  } else {
    f1<-general_lreg(vec1,vec2,deg)
    next_deg<-deg+2
    f2<-general_lreg(vec1,vec2,next_deg)
    P<-anova(f1,f2)$'Pr(>F)'[2]
    bestfit(vec1,vec2,next_deg,deg,P)
  }
}
```

Labākais polinoms:

``` {r}
res <- bestfit(ell,Cl)
paste("stopped at x^", res$d,sep="")
paste("p value:",res$p)
fitmax<-general_lreg(ell,Cl,degree=res$d,plot=T,print=T,names=c("ell","Cl"))
```

### 1.2. diagnostika

Atlikumu neatkarība - grafiski:

``` {r}
plot(ell,fit1$residuals)
plot(ell,fitmax$residuals)
```

Statistisko testu bibliotēkas:

``` {r, message=FALSE}
library(car)
library(nortest)
```

Durbin-Watson tests autokorelācijai:

``` {r}
durbinWatsonTest(fit1)
durbinWatsonTest(fitmax)
```

Normalitātes testi - grafiski:

``` {r}
qqnorm(fit1$residuals)
qqline(fit1$residuals)
qqnorm(fitmax$residuals)
qqline(fitmax$residuals)
```

\newpage
Normalitātes testi - Kolmogorova-Smirnova tests:

``` {r}
# degree-1 approximation normality
(lillie.test(fit1$residuals)$p.value > 0.05)
# max degree approximation normality
(lillie.test(fitmax$residuals)$p.value > 0.05)
```

Dispersijas vienmērīguma testi:

``` {r}
# degree-1 approximation homoscedacity
(ncvTest(fit1)$p > 0.05)
# max degree approximation homoscedacity
(ncvTest(fitmax)$p > 0.05)
```

### 1.3. secinājumi

Dati acīmredzami nav lineāri sakarīgi, un to apliecina arī visas formālās metrikas. Cits jautājums ir par reģionu [0:500], kur tie diezgan cieši seko līknei, ko labi varētu aprakstīt samērā nelielas pakāpes polinoms (sk. sekciju "atlikumu neatkarība - grafiski", kur šajā reģionā atlikumi 9. pakāpes regresijas līknei ir vienmērīgi sadalīti ap 0). Taču ap 'ell' = 500 ļoti strauji pieaug novērojumu dispersija, kas pilnībā izgāž jebkādus mēģinājumus aproksimēt visu datu kopu ar vienu līkni. Šī radikālā izmaiņa dispersijā nomāc arī jebkādus dziļākus ieskatus, ko pār visu datu kopu veiktie testi varētu sniegt par sadalījuma dabu.

\newpage

## 2. uzdevums

Datu kopas ielāde:

``` {r, message=FALSE}
df <- LifeCycleSavings
attach(df)
```

Datu kopas kolonnas:

1. sr - uzkrājumi
2. pop15 - % iedzīvotāju zem 15
3. pop75 - % iedzīvotāju virs 75
4. dpi - ienākumi
5. ddpi - IKP pieaugums

### 2.1. vispārīgu sakarību meklēšana

Izmantojot iebūvētās funkcijas cor() un pairs(), var ātri gūt vispārīgu priekštatu par datu kopā pastāvošajām sakarībām:

``` {r}
cor(df)
pairs(df)
```

Kā redzams, izteiktas sakarības nav starp nevienu parametru un uzkrājumiem, taču redzama neliela negatīva korelācija starp pop15 un sr, un nelielas pozitīvas korelācijas visos citos gadījumos.

### 2.2. regresijas modeļu konstruēšana

``` {r}
f_pop15<-general_lreg(pop15,sr,plot=T,print=T,names=c("pop15","sr"))
f_pop75<-general_lreg(pop75,sr,plot=T,print=T,names=c("pop75","sr"))
```
\newpage
``` {r}
f_dpi<-general_lreg(dpi,sr,plot=T,print=T,names=c("dpi","sr"))
f_ddpi<-general_lreg(ddpi,sr,plot=T,print=T,names=c("ddpi","sr"))
```

Kā jau vizuāli redzams, pop15 izskaidro lielāko frakciju (~20%) no sr dispersijas un ir negatīvi korelēts ar sr. Pārējie izskaidro ne vairāk kā 10%, bet ir pozitīvi korelēti.

### 2.3. regresijas modeļu nosacījumu analīze, diagnostika

Atlikumu neatkarība (grafiski):

``` {r}
plot(pop15,f_pop15$residuals,xlab="arg",ylab="residuals")
plot(pop75,f_pop75$residuals,xlab="arg",ylab="residuals")
```
\newpage
```  {r}
plot(dpi,f_dpi$residuals,xlab="arg",ylab="residuals")
plot(ddpi,f_ddpi$residuals,xlab="arg",ylab="residuals")
```

Izteiktas sakarības nav redzamas.

Atlikumu neatkarība (Durbin-Watson autokorelācijas tests, TRUE - pastāv autokorelācija):

``` {r}
(durbinWatsonTest(f_pop15)$p < 0.05)
(durbinWatsonTest(f_pop75)$p < 0.05)
(durbinWatsonTest(f_dpi)$p < 0.05)
(durbinWatsonTest(f_ddpi)$p < 0.05)
```

Atlikumu normalitāte (Kolmogorov-Smirnov tests, TRUE - normāli sadalīti):

``` {r}
(lillie.test(f_pop15$residuals)$p.value > 0.05)
(lillie.test(f_pop75$residuals)$p.value > 0.05)
(lillie.test(f_dpi$residuals)$p.value > 0.05)
(lillie.test(f_ddpi$residuals)$p.value > 0.05)
```

Dispersijas vienmērība (TRUE - dispersija nav atkarīga no argumenta)

``` {r}
(ncvTest(f_pop15)$p > 0.05)
(ncvTest(f_pop75)$p > 0.05)
(ncvTest(f_dpi)$p > 0.05)
(ncvTest(f_ddpi)$p > 0.05)
```

### 2.4. daudzfaktoru regresija

Šo apakšuzdevumu gandrīz palaidu garām, jo uzdevuma nosacījumos prasīts veikt "vienkāršas lineāras regresijas", ko var pārprast kā nosacījumu veikt individuālas viena faktora regresijas. Jebkurā gadījumā, šeit veikta daudzfaktoru regresija un rezulātu analīze kā mtcars piemērā lekcijās:

``` {r}
fit_multi<-lm(sr~., data=df)
summary(fit_multi)
```

VIF analīze:

``` {r}
vif(fit_multi)
```

Neviena vērtība netiek izmesta.

Iteratīvā uzlabošana pēc AIC metrikas:

``` {r}
fit_multi<-step(fit_multi)
summary(fit_multi)
```

Testi tāpat kā viena faktora regresijās:

``` {r}
plot(fit_multi$residuals)
(durbinWatsonTest(fit_multi)$p < 0.05)
(lillie.test(fit_multi$residuals)$p.value > 0.05)
(ncvTest(fit_multi)$p > 0.05)
```

### Secinājumi

Visos gadījumos lineārās regresijas modelis ir ne īpaši tuvs datiem, taču nav novērotas nozīmīgas autokorelācijas, atlikumi ir normāli sadalīti un to dispersijas ir vienmērīgas, kas neliecina par viegli atrodamām sistemātiskām nobīdēm. Daudzfaktoru regresijas modelis sniedz mērenu uzlabojumu (det.koef 20% -> 29%).

\newpage

## 3. uzdevums - ANOVA

Datu ielasīšana

``` {r, message=FALSE}
df <- chickwts
attach(df)
summary(df)
```

### a) Kastu grafiki, aprakstošās statistikas

Bibliotēka

``` {r, results = FALSE, message = FALSE}
library(psych)
```

Kastu grafiki, aprakstošās statistikas:

``` {r}
boxplot(weight~feed)
describe(weight[feed=="casein"])
describe(weight[feed=="horsebean"])
describe(weight[feed=="linseed"])
describe(weight[feed=="meatmeal"])
describe(weight[feed=="soybean"])
describe(weight[feed=="sunflower"])
```

### b) ANOVA modelis

``` {r}
fit<-aov(weight~feed)
summary(fit)
```

Rezultāts ir gaidītais - grupas nepieder pie viena sadalījuma.

### c) Post-hoc salīdzinājums pa pāriem

``` {r}
r<-TukeyHSD(fit)
op <- par(mar= c(4,5,3,3) + 0.1, cex.axis=0.5)
plot(TukeyHSD(fit),las=1)
par(op)
```

Grupas, kas varētu pārklāties:

* meatmeal-casein;
* sunflower-casein;
* linseed-horsebean;
* meatmeal-linseed;
* soybean-linseed;
* soybean-meatmeal;
* sunflower-meatmeal.

### d) ANOVA pieņēmumu pārbaude

Normalitāte:

``` {r, message=FALSE}
library(dplyr)
library(rstatix)
df %>% group_by(feed) %>% shapiro_test(weight)
```

Nevienai grupai nevar noraidīt.

Dispersijas vienmērība:

``` {r, message=FALSE}
leveneTest(weight,feed)
bartlett.test(weight,feed)
```

Arī to nevar noraidīt.

### e) neparametriskā ANOVA procedūra

Tā kā šajā gadījumā grupu sadalījumu normalitāti un dispersiju vienmērību noraidīt nevar, stingri runājot neparametriskas metodes nav nepieciešamas. Taču Kruskal-test procedūru var veikt jebkurā gadījumā:

``` {r}
kruskal.test(weight~feed)
```

Kā iepriekš, visu sadalījumu vienādības hipotēze tiek pārliecinoši noraidīta.

Pāru salīdzināšanai var lietot pāru Vilkoksa testu:

``` {r}
pairwise.wilcox.test(weight,feed,p.adjust.method="BH")
```

Grupas, kas varētu pārklāties:

* meatmeal-casein;
* sunflower-casein;
* meatmeal-linseed;
* soybean-linseed;
* soybean-meatmeal;
* sunflower-meatmeal.

Salīdzinot ar iepriekšējo, parametrisko metodi atkrīt:

* linseed-horsebean.

### f) neparametriskais post-hoc tests

Izmantojot Dunn testu:

``` {r}
dunn_test(df, weight~feed)
```

Grupas, kuru pārklāšanos nevar izslēgt:

* meatmeal-casein;
* soybean-casein;
* sunflower-casein;
* linseed-horsebean;
* soybean-horsebean;
* meatmeal-linseed;
* soybean-linseed;
* soybean-meatmeal;
* sunflower-meatmeal;
* sunflower-soybean.

Salīdzinot ar parametrisko metodi, klāt nākušas:

* soybean-casein;
* soybean-horsebean;
* sunflower-soybean.

### g) secinājumi un komentāri

* Kaut gan ANOVA normalitātes un dispersijas vienmērīguma nosacījumi it kā reti izpildās, šajā datu kopā nekādas problēmas tie nesagādā;
* Dažas grupas ļoti robusti iztur vienādības pārbaudes visās testu kategorijās, bet citas ir uz robežas - dažādi testi sniedz dažādas atbildes - kaut gan tās ir ~5% p-vērtības visos variantos.

\newpage
## 4. uzdevums - 2-faktoru ANOVA

Ceru, ka esmu pareizi sapratis uzdevuma nosacījumus, jo šķiet, ka šajā punktā jādara daudz mazāk nekā citos.

Datu ielasīšana:

``` {r}
df <- ToothGrowth
attach(df)
summary(df)
```

Datu kolonnas:

* len - zobu garums;
* supp - uztura bagātinātājs (2 kategorijas);
* dose - doza (3 līmeņi - {0.5, 1, 2}).

2-faktoru ANOVA novērtējums:

``` {r}
summary(aov(len ~ supp*dose))
```

ANOVA tests liecina, ka katra kategorija ir statistiski nozīmīga (p -> 0). Abu kategoriju mijiedarbība izdalīta kā neatkarīgs faktors nav ne tuvu tik nozīmīga kā katrs atsevišķi, taču p-vērtība tik un tā ir zem 5%, kas ļauj šo mijiedarbību arī atzīt par statistiski nozīmīgu. 

Izmantojot HSD.test, var redzēt, kā kategorijas grupējas:

```{r}
library(agricolae)
tx<-with(df, interaction(supp,dose))
amod <- aov(len ~ tx)
HSD.test(amod, "tx", group=TRUE,console=TRUE)
```

Grupās, kas izdalāmas arī ar grafisko metodi, kas apskatīta lekcijā, taču konfliktē ar Markdown kompilatoru, redzams, ka pieaugot dozai, mazinās atšķirības starp uztura bagātinātājiem.

\newpage

## 5. uzdevums - joslas platuma meklēšana

Datu ielāde (dots speciāls 5-modāls sadalījums, kas lekcijās izmantots, lai ilustrētu pdf novērtējuma metožu trūkumus):

``` {r}
df <- as.numeric(read.delim("dati2_5.txt",header=F,sep=" "))
```

### 5.1. uzdevums - joslas platuma atrašana

Izmantojot R iebūvēto krosvalidācijas optimizatoru:

``` {r}
cross_validated<-bw.ucv(df)
cross_validated
```

### 5.2. ilustrēt KDE aproksimāciju pret histogrammu

Lai uzskatāmāk parādītu arī "nepiegludinātus" un "pārgludinātus" PDF tuvinājumus, zīmētas līknes ar h = 1 un h = 0.01:

``` {r, results=F}
library(histogram)
hh<-histogram(df,type="regular",penalty="cv")
lines(density(df,bw=1),col="blue",lwd=2)
lines(density(df,bw=0.01),col="green",lwd=2)
lines(density(df,bw="ucv"),col="red",lwd=3)
```

### 5.3. salīdzināt ar citām joslas platuma novērtējuma metodēm

Grāmatā "Nonparametric and Semiparametric Models" dota formula t.s. "Silverman rule of thumb" heiristikai, kas pieņem normālā sadalījuma otro atvasinājumu un līdz ar to ļauj iegūt joslas platumu no datu kopas standartnovirzes novērtējuma. Šo un nedaudz paplašinātu heiristiku arī piedāvā R iebūvētais joslas platuma noteikšanas rīks: 

``` {r}
bw.nrd0(df)
bw.nrd(df)
```

Kā redzams, iegūtie rezultāti ir ļoti līdzīgi. Salīdzinot tuvāko (mazāko) ar krosvalidācijas optimizatora iegūto:

``` {r, results=F}
hh<-histogram(df,type="regular",penalty="cv")
lines(density(df,bw="ucv"),col="red",lwd=3)
lines(density(df,bw="nrd0"),col="black",lwd=3)
```

Kā redzams, ar krosvalidācijas metodi iegūtā KDE līkne labāk seko īstajam sadalījumam.

\newpage
## 6. uzdevums - neparametriskā regresija

Datu ielāde:

``` {r, results=F, message=F}
df <- read.table('CMB.dat',header=TRUE)
attach(df)
```

### 6.1. regresiju ģenerēšana

Polinomiālā regresija ar jau iepriekš noskaidroto "labāko" vērtību:

``` {r}
p <- lm(Cl~poly(ell,9,raw=T))
```

Neparametriskā regresija ar NW metodi bez papildus korekcijām (pirmā funkcija nosaka joslas platumu, otrā ģenerē pašu regresijas līkni):

``` {r, message=F, results=F}
library(np)
bw <- npregbw(Cl~ell,bwmethod="cv.ls",regtype="lc")
n <- npreg(bw,residuals=T)
```

Grafisks attēlojums:

``` {r}
plot(ell,Cl,col="grey",ylim=c(-500,6000))
lines(n$mean,col="red",lwd=2)
lines(p$fitted.values,col="blue",lwd=2,lty="dashed")
```

\newpage
### 6.2. nosacījumu pārbaude, salīdzinājums

Grafiski attēlojumi:

``` {r, echo=F}
op<-par(mfrow = c(1,2))
plot(ell,n$resid,ylim=c(-2000,2000))
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,p$residuals,ylim=c(-2000,2000))
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,n$resid)
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,p$residuals)
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
acf(n$resid)
acf(p$residuals)
par(op)
```

Grafiski redzams, ka definīcijas intervālā polinoms labāk tuvināts datiem, taču asi maina virzienu abos galos, kas to padara nederīgu paredzējumiem ārpus intervāla. Autokorelācijas abos gadījumos ir diezgan līdzīgas, taču neparametriskās regresijas gadījumā grafiski redzams, ka saglabājusies sistemātiska, periodiska nobīde starp modeli un datiem. Polinoma gadījumā atlikumi veido (vismaz vizuāli) nejauši izkliedētu punktu mākoni.

Jau pirmajā uzdevumā noskaidrots, kas neizpildās ne normalitātes, ne dispersijas vienmērīguma nosacījumi atlikumos, kas nepieciešami lineārām un polinomiālām regresijām.

### 6.3. joslas platuma novērtējumi

NW nav vienīgā metode, ar kuru var ģenerēt neparametriskas regresijas. Lai iegūtu joslas platumus, var izmantot t.s. "plug-in" metodes, kas izmanto vienkāršotas heiristikas. Kodola regresijas jēdzienu iespējams vispārināt, padarot izteiksmi par polinomu katrā punktā - iegūstot lokāli lineāru vai polinomiālu modeli. 

``` {r, results=F}
bwll <- npregbw(Cl~ell,regtype="ll",bwmethod="cv.aic")
# would be dpill(ell,Cl), but wilcox test crashes the compiler.
bwrt <- 34.61873 
bwrt <-npregbw(Cl~ell,bws=bwrt,bandwidth.compute=F)
```
``` {r}
bwll
bwrt
```

Salīdzinot regresijas metodes grafiski:

``` {r}
nll <- npreg(bwll,residuals=T)
nrt <- npreg(bwrt,residuals=T)
```

NW pret lokāli polinomiālo regresiju (metode ir ekvivalenta locpoly funkcijai, bet bez patvaļīgās argumentu kopas pārkārtošanas):

``` {r}
plot(ell,Cl,col="grey",ylim=c(0,5500))
lines(n$mean,col="black",lwd=6)
lines(nll$mean,col="red",lwd=3)
``` 

Krosvalidācija pret "plug-in" kodola aplēšanu:

``` {r}
plot(ell,Cl,col="grey",ylim=c(0,5500))
lines(n$mean,col="black",lwd=6)
lines(nrt$mean,col="red",lwd=3)
```

Redzams, kas joslas platuma izvēles metodei ir minimāls iespaids uz rezultējošo līkni šajā gadījumā, taču lokāli polinomiālā regresija datu kopas galos uzvedas ievērojami savādāk nekā parastā NW metode.

Salīdzinot atlikumus:

```{r, echo=F}
op<-par(mfrow = c(1,2))
plot(ell,n$resid,ylim=c(-2000,2000))
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,nll$resid,ylim=c(-2000,2000))
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,n$resid)
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
plot(ell,nll$resid)
abline(h=0,col="black",lwd=4)
abline(h=0,col="white",lty="dashed",lwd=2)
acf(n$resid)
acf(nll$resid)
par(op)
```

Redzams, ka mazliet mazinājusies pārpalikusī struktūra atlikumos, taču tuvinājums vēl joprojām nav tik labs kā polinoma gadījumā.

\newpage
## 7. uzdevums - bootstrap, robusti novērtētāji

Datu ielasīšana:

``` {r}
df <- InsectSprays
attach(df)
```

### 7.1. dažādu statistiku ticamības intervāli

Funkcijas statistiku aprēķinam:

``` {r, message=F}
library(boot)
library(e1071)
cb_mean <- function(x, i){
  mean(x[i])
}
cb_median <- function(x, i){
  median(x[i])
}
cb_skewness <- function(x, i){
  skewness(x[i])
}
```

Statistiku ticamības intervāli:

``` {r}
N<-10000
bobj_mean <- boot(count,statistic=cb_mean,R=N)
bobj_medi <- boot(count,statistic=cb_median,R=N)
bobj_skew <- boot(count,statistic=cb_skewness,R=N)
boot.ci(bobj_mean)
boot.ci(bobj_medi)
boot.ci(bobj_skew)
```

Salīdzinājums ar parametrisku un neparametrisku testu:

``` {r}
t.test(count,mu=mean(count))
wilcox.test(count,conf.int = T)
```

### 7.2. robustie lokācijas novērtējumi

Vidējā vērtība pēc tradicionālā aprēķina:

``` {r}
mean(count)
```

\newpage
Robustas metodes:

```{r, message=F}
library(robustbase)
```
``` {r}
median(count)
mean(count,trim=0.2)
huberM(count)$mu
```

Secinājumi: Hubera M-novērtējums un "nogrieztā" sadalījuma vidējā vērtība ir samērā tuvi vidējai, taču mediāna ir ievērojami atšķirīga. 

\newpage
## 8. uzdevums - robustā regresija

Datu ielāde:

``` {r}
library(robustbase)
df<-coleman
attach(df)
summary(df)
```

Datu apraksts - valodas testu rezultāti skolās:

- Y - rezultāti valodas testā;
- salaryP - darbinieku algas uz vienu skolēnu;
- fatherWc - to tēvu frakcija, kas strādā "white collar" darbus;
- sstatus - socio-ekonomiskā statusa novērtējums;
- teacherSc - skolotāju testu rezultāti;
- motherLev - mātes izglītības līmenis;

### 8.1 aprakstošās statistikas:

``` {r, fig.height=3}
library(psych)
describe(df$Y)
boxplot(df$Y)
hist(df$Y)
describe(df$salaryP)
boxplot(df$salaryP)
hist(df$salaryP)
describe(df$fatherWc)
boxplot(df$fatherWc)
hist(df$fatherWc)
describe(df$sstatus)
boxplot(df$sstatus)
hist(df$sstatus)
describe(df$teacherSc)
boxplot(df$teacherSc)
hist(df$teacherSc)
describe(df$motherLev)
boxplot(df$motherLev)
hist(df$motherLev)
```

### 8.2. korelācijas

Korelācijas starp Y un argumentiem:

``` {r}
correlations<-cor(df)
correlations[6,]
```

Redzams, ka socio-ekonomiskais statuss, tēva darbavietas veids un mātes izglītības līmenis ir savstarpēji ļoti cieši korelēti:

``` {r}
correlations[3,]
```

### 8.3. daudzfaktoru lineārā regresija

Regresija:

``` {r, message=F}
library(car)
```
``` {r}
fit_multi<-lm(Y~., data=df)
summary(fit_multi)
vif(fit_multi)
fit_multi<-step(fit_multi)
summary(fit_multi)
```

Pārbaudes - grafiski (īsi secinājumi komentāru formā):

``` {r, fig.height=3}
plot(fit_multi)
# Residuals vs fitted: Minor visually apparent correlation
# Normal QQ: Two significant oultiers, mostly normally distributed residuals
# Scale-Location: not much apparent heteroscedacity
# Leverage: No outliers with leverage
acf(fit_multi$residuals)
# some apparent autocorrelation
```

Pārbaudes - statistiku testi:

```{r, message=F}
library(nortest)
```
``` {r}
p<-0.05
# Are the residuals autocorrelated?
(durbinWatsonTest(fit_multi)$p < p)
# Are the residuals normally distributed?
(lillie.test(fit_multi$residuals)$p.value > p)
# Are the residuals homoscedastic?
(ncvTest(fit_multi)$p > p)
```

### 8.4. daudzfaktoru robustā regresija

Regresija:

```{r}
fit<-lmrob(Y~.,data=df)
summary(fit)
```

Novērtējums (tikai grafiski, jo nav atbalsta lmrob objektam; secinājumi komentāros):

```{r, fig.height=3}
plot(fit)
# Residuals vs leverage: One outlier with leverage (point 18)
# Normal QQ vs residuals: All error terms besides outliers normally distributed
# Response vs fitted: Two significant outliers, otherwise close fit
# Residuals vs fitted: No clear pattern, can assume residuals are IID
# Sqrt of residuals: can see some heteroscedacity (increasing with Y)

```

Salīdzinājums: robustajai regresijai augstāks determinācijas koeficients (97% vs 88%), pat neizmetot savstarpēji korelētos parametrus.
